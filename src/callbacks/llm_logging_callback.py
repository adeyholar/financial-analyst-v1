# src/callbacks/llm_logging_callback.py
import logging
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
import json
# NEW IMPORTS FOR TYPE HINTING
from typing import List, Any, Union

logger = logging.getLogger(__name__)

class LLMLoggingCallbackHandler(BaseCallbackHandler):
    """
    Custom LangChain Callback Handler for logging LLM requests and responses.
    """

    def on_chat_model_start(self, serialized: dict, messages: List[List[BaseMessage]], **kwargs: Any) -> Any:
        """
        Called when a chat model is about to start running.
        Logs the input messages to the LLM.
        """
        logged_messages = []
        for chat_messages in messages:
            for msg in chat_messages:
                if hasattr(msg, 'content'):
                    logged_messages.append({'type': msg.__class__.__name__, 'content': msg.content})
                else:
                    logged_messages.append(f"<Unrecognized message type: {msg.__class__.__name__}>")

        try:
            logger.debug(f"LLM Request (on_chat_model_start): Messages={json.dumps(logged_messages, indent=2)}")
        except Exception as e:
            logger.warning(f"Could not fully serialize LLM Request for logging in callback: {e}")
            logger.debug(f"LLM Request (on_chat_model_start, partial): Messages={str(logged_messages)[:500]}...")

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """
        Called when a new token is generated by the LLM.
        We won't log every token to avoid excessive output, but you could add it.
        """
        pass # Not logging individual tokens for brevity

    def on_llm_end(self, response: Any, **kwargs: Any) -> Any:
        """
        Called when the LLM has finished running.
        Logs the LLM's response.
        """
        logged_response = None
        if hasattr(response, 'content'): # For ChatMessage
            logged_response = {'type': response.__class__.__name__, 'content': response.content}
        elif hasattr(response, 'generations') and response.generations: # For LLMResult
            gen_contents = []
            for gen_list in response.generations:
                for gen in gen_list:
                    if hasattr(gen, 'text'):
                        gen_contents.append(gen.text)
                    elif hasattr(gen, 'message') and hasattr(gen.message, 'content'):
                        gen_contents.append(gen.message.content)
            logged_response = {'generations': gen_contents}
        else:
            logged_response = f"<Unrecognized response type or structure: {type(response).__name__}>"

        try:
            logger.debug(f"LLM Response (on_llm_end): {json.dumps(logged_response, indent=2)}")
        except Exception as e:
            logger.warning(f"Could not fully serialize LLM Response for logging in callback: {e}")
            logger.debug(f"LLM Response (on_llm_end, partial): {str(logged_response)[:500]}...")

    def on_llm_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> Any:
        """
        Called when an LLM error occurs.
        """
        logger.error(f"LLM Error (on_llm_error): {error}")